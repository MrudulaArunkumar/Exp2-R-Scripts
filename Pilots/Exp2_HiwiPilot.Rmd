---
title: "Exp2_pilotdata_hiwis"
author: "Mrudula"
date: "20/10/2020"
output: html_document
---

This is the Pilot Data Analysis collected from the hiwis for the second version of the Experiment that brought back the distractor words towards the center and the number appears either at the top or bottom.

1.  Loading the libraries and the relevant data files

```{r library and files, include=FALSE, message=FALSE}
sessionInfo()#saving the R session

library(tidyverse)
library(plyr)
library(ez)
library(schoRsch)
library(knitr)
library(pander)
library(rmarkdown)

#clearing environment
rm(list = ls())

#setting up working directory globally only needed when using it first time
library(here)
set_here()

#loading pilot
d1 <- read.csv("D:/PhD/Experiments/Exp2 Overshadowing/Data/Pilot/1_pilotv2.csv")
d2 <- read.csv("D:/PhD/Experiments/Exp2 Overshadowing/Data/Pilot/2_pilotv2.csv")
d3 <- read.csv("D:/PhD/Experiments/Exp2 Overshadowing/Data/Pilot/3_pilotv2.csv")
d4 <- read.csv("D:/PhD/Experiments/Exp2 Overshadowing/Data/Pilot/4_pilotv2.csv")

#adding columns to be able to rbind. since these were older versions
d1$preResp.rt <- 0
d1$PROLIFIC_PID <- NA

d2$PROLIFIC_PID <- 0
d4$preResp.rt <- 0

Exp2_HiwiPilot <- rbind(d1,d2,d3,d4)



#write.csv(Exp2_HiwiPilot, file = "HiwiPilot.csv")
attach(Exp2_HiwiPilot)



```

2.  Checked balance of trials in design and all are good

```{r design Check, include = FALSE}
#blockcount
table(Block)
table(Block,Condition) #29 refers to the NA rows
table(Block,Condition, Validity)

#position
table(PositionD)
table(PositionT)
table(Saliency)

detach(Exp2_HiwiPilot)
```

Cleaning data: removing unnecessary columns, filling the screen background for all cells, renaming the blocks, adjusting the rows containing RTs (splitting them and showing in units of ms) and creating a column for errorRate

```{r cleaning, include=FALSE}

Exp2_HiwiPilot <- Exp2_HiwiPilot %>%
  select(-consentkey.keys,-consentkey.rt,-beginexp.rt,-beginexp.keys,-checkresp.corr,-checkresp.rt,-checkresp.keys,
         -Attention.ran,-Attention.thisIndex,-Attention.thisN,-Attention.thisTrialN,-Attention.thisRepN,
         -Question,-Solution,-gotoPrac.rt,-gotoPrac.keys,-InstRep.ran,-InstRep.thisIndex,-InstRep.thisN,-InstRep.thisTrialN,-InstRep.thisRepN,
         -Prac_start.rt,-Prac_start.keys,-prctrials.ran,-prctrials.thisIndex,-prctrials.thisN,-prctrials.thisRepN,-prctrials.thisRepN,
         -pracend.keys,-pracend.rt,-PracRepeat.ran,-PracRepeat.thisIndex,-PracRepeat.thisN,-PracRepeat.thisRepN,-PracRepeat.thisTrialN)

#removing unwanted columns in the experimental side 
Exp2_HiwiPilot <- Exp2_HiwiPilot %>%
  select(-firstlearntrials.ran,-firstlearntrials.thisIndex,-firstlearntrials.thisRepN,-firstlearntrials.thisTrialN,
         -brkcontinue.keys,-Exptrials.ran,-Exptrials.thisIndex,-Exptrials.thisTrialN, - Exptrials.thisRepN,
         -afterpause.keys,-blocks.ran,-blocks.thisIndex,-blocks.thisTrialN,-blocks.thisRepN,
         -CAproceed.rt,-CAproceed.keys,-ContAwareness.ran,-ContAwareness.thisIndex,-ContAwareness.thisN,-ContAwareness.thisTrialN,-ContAwareness.thisRepN,
         -Questionnaire.ran,-Questionnaire.thisIndex,-Questionnaire.thisTrialN,-Questionnaire.thisRepN,-todebrief.keys,-ExpExit.keys)


#assigning the vaues of blockcount and screenbackground for every row

Exp2_HiwiPilot$Screen_bg <- as.character(Exp2_HiwiPilot$Screen_bg)
Exp2_HiwiPilot <- Exp2_HiwiPilot%>%dplyr::group_by(participant)%>%tidyr::fill(Screen_bg,.direction = "down")

Exp2_HiwiPilot <- Exp2_HiwiPilot %>% group_by(participant)%>%fill(blocks.thisN,.direction = "up")

Exp2_HiwiPilot <- rename(Exp2_HiwiPilot, c("blocks.thisN" = "BlockCount"))

Exp2_CA <- Exp2_HiwiPilot %>%
  filter(Condition == "ContChkTest" | str_detect(AwareQ, "mit"))


#adjusting RT
Exp2_HiwiPilot <- separate(Exp2_HiwiPilot, col = ResponseKey.rt, into = c("RT_Trials", "RT_secondary"), sep = ',')
Exp2_HiwiPilot$RT_Trials <- Exp2_HiwiPilot$RT_Trials%>%
  str_replace_all("\\[|\\]","")%>%
  as.double(Exp2_HiwiPilot$RT_Trials)
Exp2_HiwiPilot$RT_Trials <- 1000*(Exp2_HiwiPilot$RT_Trials)
Exp2_HiwiPilot$PreTargetDisplayTime <- 1000*(Exp2_HiwiPilot$PreTargetDisplayTime)

Exp2_HiwiPilot <- Exp2_HiwiPilot%>%drop_na(RT_Trials)

Exp2_HiwiPilot$ACC_trials <- Exp2_HiwiPilot$ResponseKey.corr
Exp2_HiwiPilot$ErrorRate <- 1 - Exp2_HiwiPilot$ACC_trials

```

Summary of the overall RT

```{r descriptive, echo=FALSE}
pander(summary(Exp2_HiwiPilot$RT_Trials), style = 'rmarkdown',caption = 'Mean RT')
pander(table(Exp2_HiwiPilot$ACC_trials),style = 'rmarkdown',caption = "Accuracy")

```

Removing outliers and farouts and tabulating the summary of RTs after removing outliers/farouts

```{r outliersfarouts, include=FALSE}

Exp2_HiwiPilot$RT_Trials[Exp2_HiwiPilot$ACC_trials==0] <- NA


#creating function to remove the outliers and farouts
computeTukeys <- function(x){
  P25 <- quantile(x$RT_Trials, .25, na.rm = TRUE, type = 6) #type = 6 -> used in SPSS
  P75 <- quantile(x$RT_Trials, .75, na.rm = TRUE, type = 6)
  x$Outlier <- P75 + 1.5*(P75 - P25)
  x$Farouts <- P75 + 3.0*(P75 - P25)
  return(x)
}


#identifying the outliers and farouts at individual level
Exp2_HiwiPilot <- ddply(Exp2_HiwiPilot, .(participant), computeTukeys)

#creating new column with RT trials after removing outliers/farouts
Exp2_HiwiPilot$RT_ifo <- Exp2_HiwiPilot$RT_Trials
Exp2_HiwiPilot$RT_io <- Exp2_HiwiPilot$RT_Trials
Exp2_HiwiPilot$RT_ifo[Exp2_HiwiPilot$RT_ifo > Exp2_HiwiPilot$Farouts|Exp2_HiwiPilot$RT_ifo < 300] <- NA
Exp2_HiwiPilot$RT_io[Exp2_HiwiPilot$RT_io > Exp2_HiwiPilot$Outlier|Exp2_HiwiPilot$RT_io < 300] <- NA

pander(summary(Exp2_HiwiPilot$RT_ifo), style = 'rmarkdown', caption = "Summary of RT after removing Farouts")
pander(summary(Exp2_HiwiPilot$RT_io), style = 'rmarkdown', caption = "Summary of RT after removing Outliers")


```



## Analysis of Validity across all trials

    There are no main effects, however there is a significant interaction between the type of a trial         (learn/test) and validity. As per the plot it also looks like for test trials, valid trials are faster        but for learn valid trials, they are slower. 
      
```{r alltrials, echo = FALSE, warning=FALSE}
Exp2agg <- aggregate(data = Exp2_HiwiPilot,RT_ifo~participant+Validity+Condition,mean)


anova_agg <- ezANOVA(data = Exp2agg,
        dv = RT_ifo,
        wid = participant,
        within = .(Validity,Condition),
        detailed = TRUE)

pander(anova_agg, style = "markdown", caption = "ANOVa table for all trials with validity and condition as factors")

ezPlot(data = Exp2agg,
        dv = RT_ifo,
        wid = participant,
        within = .(Validity,Condition),
       x=Condition,split = Validity, do_bars = FALSE)+
  ylim(550,600)+theme_classic()
```
### Splitting as learn and test trials to analyze separately

The data are Split as learn and test trials to analyse the effects of validity and saliency separately.

```{r}
Exp2learn_HiwiPilot <- Exp2_HiwiPilot %>%
  filter(Condition == "learn")

Exp2test_HiwiPilot <- Exp2_HiwiPilot %>%
  filter(Condition == "test")


#creating a variable taht refers to whether the position of the salient word and the target was the same or not.
Exp2learn_HiwiPilot<- Exp2learn_HiwiPilot%>%
  mutate(PositionMatch = ifelse(as.character(PositionD)==as.character(PositionT),"same","different"))

```

## Analysis for learn trials

-   Farouts
    
    Simple t test to compare valid and invalid trials and there are no significant differences between them

```{r learnfo, echo = FALSE, warning=FALSE}
Exp2learn_HiwiPilot <- Exp2learn_HiwiPilot %>%
  select(Validity,Saliency,Condition,PositionD,PositionT,Distractor1,Distractor2,everything())

#aggregate
Exp2agg_l_hp_fo <- aggregate(data = Exp2learn_HiwiPilot,RT_ifo~participant+Validity,mean)
kable(Exp2agg_l_hp_fo, format = "html", caption = "Mean of valid and invalid learn trials")

t.test(RT_ifo~Validity, data = Exp2agg_l_hp_fo, paired = TRUE)

```

-   Outliers

    Again no difference between valid and invalid learn trials

    ```{r learno, echo = FALSE}

    #aggregate
    Exp2agg_l_hp_o <- aggregate(data = Exp2learn_HiwiPilot,RT_io~participant+Validity,mean)
    kable(Exp2agg_l_hp_o, format =  "html", caption = "Mean of RTs(Outliers excluded) of valid and invalid trials")

    t.test(RT_io~Validity, data = Exp2agg_l_hp_o, paired = TRUE)

    ```

-   Error Rate

There is a main effect but in the opposite direction where there are more errors for valid trials compared to invalid trials

```{r errorl, echo = FALSE,warning=FALSE}

Exp2agg_l_hp_ER <- aggregate(data = Exp2learn_HiwiPilot,ErrorRate~participant+Validity,mean)
kable(Exp2agg_l_hp_ER, format = "html", caption = "Summary means for Error RATEs")

anova_t_ER <- ezANOVA(data = Exp2agg_l_hp_ER,
        dv = ErrorRate,
        wid = participant,
        within = .(Validity),
        detailed = TRUE)

pander(anova_t_ER, style = "markdown", caption = "ANOVa table for ERROR RATE in learn trials")
t.test(ErrorRate~Validity, data = Exp2agg_l_hp_ER, paired = TRUE)

```

## Analysis of Test Trials

-   Farouts

      In the pilot data from Hiwis, there is still no significant effect but the validity is almost significant with p = .10 Also in the right direction as per the graph

```{r testfo, warning=FALSE}
Exp2test_HiwiPilot <- Exp2test_HiwiPilot %>%
  select(Validity,Saliency,Condition,PositionD,PositionT,Distractor1,Distractor2,everything())

#aggregate
Exp2agg_t_hp_fo <- aggregate(data = Exp2test_HiwiPilot,RT_ifo~participant+Validity+Saliency,mean)

aggmean_t_fo <- ddply(Exp2agg_t_hp_fo, .(participant,Validity), summarize, RTmean=mean(RT_ifo), SD = sd(RT_ifo))
salmean_t_fo <- ddply(Exp2agg_t_hp_fo, .(participant,Saliency), summarize, RTmean=mean(RT_ifo), SD = sd(RT_ifo))

kable(aggmean_t_fo, format = "html", caption = "Summary means of valid vs invalid across participant")
kable(salmean_t_fo, format = "html", caption = "Summary means of Salient vs non salient across participant")

anova_t_fo <- ezANOVA(data = Exp2agg_t_hp_fo,
        dv = RT_ifo,
        wid = participant,
        within = .(Saliency,Validity),
        detailed = TRUE)

pander(anova_t_fo, style = 'rmarkdown', caption = "ANOVA results: Farouts excluded for test trials")

ezPlot(data = Exp2agg_t_hp_fo,
        dv = RT_ifo,
        wid = participant,
        within = .(Saliency,Validity),
        split = Validity, x=Saliency, do_bars = FALSE)+
        theme_classic()+
        ylim(550,600)+
        ggtitle("Mean RT for valid and invalid trials across saliency")
```

-   Outliers

Here, the effects are not significant, but atleast the direction seems to be as expected where the mean difference between valid and invalid is larger for salient trials than non salient trials. But not statistically significant.

```{r testo, warning=FALSE}
Exp2agg_t_hp_o <- aggregate(data = Exp2test_HiwiPilot,RT_io~participant+Validity+Saliency,mean)
means_t_o <- aggregate(data=Exp2agg_t_hp_o,RT_io~participant+Validity,mean)
salmeans_t_o <- aggregate(data=Exp2agg_t_hp_o,RT_io~participant+Saliency,mean)


kable(means_t_o, format = "html", caption = "Summary means of valid vs invalid across participant")
kable(salmeans_t_o, format = "html", caption = "Summary means of Salient vs non salient across participant")


anova_t_o <- ezANOVA(data = Exp2agg_t_hp_o,
            dv = RT_io,
            wid = participant,
            within = .(Saliency,Validity),
            detailed = TRUE)
pander(anova_t_o, style = 'rmarkdown', caption = "ANOVA results: Outliers excluded for test trials")
 
ezPlot(data = Exp2agg_t_hp_o,
        dv = RT_io,
        wid = participant,
        within = .(Saliency,Validity),
        split = Validity, x=Saliency, do_bars = FALSE)+
        ylim(550,600)+
        theme_classic()

```

-   Error Rate

    No effects in the error Rates

```{r ert, echo = FALSE, warning = FALSE}

Exp2agg_t_hp_ER <- aggregate(data = Exp2test_HiwiPilot,ErrorRate~participant+Validity+Saliency,mean)

Exp2agg_t_hp_ER$ErrorRate <- 100 * Exp2agg_t_hp_ER$ErrorRate

anova_t_ER <- ezANOVA(data = Exp2agg_t_hp_ER,
        dv = ErrorRate,
        wid = participant,
        within = .(Saliency,Validity),
        detailed = TRUE)

pander(anova_t_ER, style = 'rmarkdown', caption = "ANOVA results: ErrorRates in test trials")


```

### Exploratory: Adding the factor of Position

      This analysis discusses the effect of salient word and the position of the number. Position Match variable indicates whether the salient word position and number appeared in the same place or not. Although this may not be of help because we brought the words closer together, I added this factor anyway to see how the data is.

#### Analysis of learn trials - Position

-   Farouts

      There seems to be a trend that is heading towards a possible significant interaction between validity and the position of the number/salient word in the LEARN Trials. However the direction seems reversed, wherein the valid trials with same match of position of number and salient word are the slowest.

```{r learnp, echo = FALSE, warning=FALSE}


#aggregate
Exp2agg_lp_hp_fo <- aggregate(data = Exp2learn_HiwiPilot,RT_ifo~participant+Validity+PositionMatch,mean)


lp_anova <- ezANOVA(data = Exp2agg_lp_hp_fo,
        dv=RT_ifo,
        wid = participant,
        within = .(Validity, PositionMatch),
        detailed = TRUE)
pander(lp_anova,style = 'rmarkdown')

ezPlot(data = Exp2agg_lp_hp_fo,
        dv=RT_ifo,
        wid = participant,
        within = .(Validity, PositionMatch),
       split = Validity, x=PositionMatch, do_bars = FALSE)+
      theme_classic()+
        ylim(550,600)

```

-   Outliers

      This also shows a similar trend as the farouts, again in the reversed direction.

    ```{r learnop, echo = FALSE, warning=FALSE}
    Exp2agg_lp_hp_o <- aggregate(data = Exp2learn_HiwiPilot,RT_io~participant+Validity+PositionMatch,mean)


    lp_anova_o <- ezANOVA(data = Exp2agg_lp_hp_o,
        dv=RT_io,
        wid = participant,
        within = .(Validity, PositionMatch),
        detailed = TRUE)
    pander(lp_anova_o,style = 'rmarkdown')

    ezPlot(data = Exp2agg_lp_hp_o,
        dv=RT_io,
        wid = participant,
        within = .(Validity, PositionMatch),
       split = Validity, x=PositionMatch, do_bars = FALSE)+
        ylim(550,600)+
        theme_classic()

    ```

-   Error Rate

      There is a significant effect of error rate but with a reversed trend where valid trials have higher error rate (although the difference is not much)

```{r errorlp, echo = FALSE, warning=FALSE}

Exp2agg_lp_hp_ER <- aggregate(data = Exp2learn_HiwiPilot,ErrorRate~participant+Validity+PositionMatch,mean)

lp_anova_er <- ezANOVA(data = Exp2agg_lp_hp_ER,
        dv=ErrorRate,
        wid = participant,
        within = .(Validity, PositionMatch),
        detailed = TRUE)
pander(lp_anova_er,style = 'rmarkdown')

ezPlot(data = Exp2agg_lp_hp_ER,
        dv=ErrorRate,
        wid = participant,
        within = .(Validity, PositionMatch),
       split = Validity, x = PositionMatch, do_bars = FALSE)
```

#### Analysis of test trials - Position

-   Farouts When position of the number is factored in the validity is almost significant (p = .09), also in the direction where top position is faster

```{r testp, echo = FALSE}

#aggregate
Exp2agg_tp_hp_fo <- aggregate(data = Exp2test_HiwiPilot,RT_ifo~participant+Validity+Saliency+PositionT,mean)


tp_anova <- ezANOVA(data = Exp2agg_tp_hp_fo,
        dv=RT_ifo,
        wid = participant,
        within = .(Validity, PositionT),
        detailed = TRUE)
pander(tp_anova,style = 'rmarkdown')

ezPlot(data = Exp2agg_tp_hp_fo,
        dv=RT_ifo,
        wid = participant,
        within = .(Validity, PositionT),
       split = Validity, x=PositionT, do_bars = FALSE)+
        ylim(550,600)+
        theme_classic()

```

-   Outliers

    There seems to be a trend heading towards a significant interaction between validity and position of number. but strangely the trend in validity does not match how it appeared to be in farouts. 

    ```{r testop, echo = FALSE}
    Exp2agg_tp_hp_o <- aggregate(data = Exp2test_HiwiPilot,RT_io~participant+Validity+PositionT,mean)


    tp_anova_o <- ezANOVA(data = Exp2agg_tp_hp_o,
        dv=RT_io,
        wid = participant,
        within = .(Validity, PositionT),
        detailed = TRUE)
    pander(tp_anova_o,style = 'rmarkdown')

    ezPlot(data = Exp2agg_tp_hp_o,
        dv=RT_io,
        wid = participant,
        within = .(Validity, PositionT),
       split = Validity, x=PositionT, do_bars = FALSE)+
        ylim(550,600)+
        theme_classic()

    ```

-   Error Rate

    No effect

```{r errortp, echo = FALSE}

Exp2agg_tp_hp_ER <- aggregate(data = Exp2test_HiwiPilot,ErrorRate~participant+Validity+PositionT,mean)

tp_anova_er <- ezANOVA(data = Exp2agg_tp_hp_ER,
        dv=ErrorRate,
        wid = participant,
        within = .(Validity, PositionT),
        detailed = TRUE)
pander(tp_anova_er,style = 'rmarkdown')
```

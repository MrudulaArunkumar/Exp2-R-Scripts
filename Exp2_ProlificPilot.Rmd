---
title: "Exp2_pilotdata_prolific"
author: "Mrudula"
date: "20/10/2020"
output: html_document
---

This is the Pilot Data Analysis collected from the hiwis for the second version of the Experiment that brought back the distractor words towards the center and the number appears either at the top or bottom.

Loading the libraries and the relevant data files

```{r library and files, include=FALSE, message=FALSE}
sessionInfo()#saving the R session

library(tidyverse)
library(plyr)
library(ez)
library(schoRsch)
library(knitr)
library(pander)
library(rmarkdown)

#clearing environment
rm(list = ls())

#setting up working directory globally only needed when using it first time
library(here)
set_here()

#loading pilot
d1 <- read.csv("D:/PhD/Experiments/Exp2 Overshadowing/Data/Pilot/Prolific Pilot/1pp.csv")
d2 <- read.csv("D:/PhD/Experiments/Exp2 Overshadowing/Data/Pilot/Prolific Pilot/8pp.csv")
d3 <- read.csv("D:/PhD/Experiments/Exp2 Overshadowing/Data/Pilot/Prolific Pilot/4pp.csv")
d4 <- read.csv("D:/PhD/Experiments/Exp2 Overshadowing/Data/Pilot/Prolific Pilot/6pp.csv")
d5 <- read.csv("D:/PhD/Experiments/Exp2 Overshadowing/Data/Pilot/Prolific Pilot/7pp.csv")

table(d4$ResponseKey.corr)

d1$preResp.rt <- 0
d2$preResp.rt <- 0

#d4 has too many errors
Exp2_ProlificPilot <- rbind(d1,d2,d3,d5)

table(Exp2_ProlificPilot$preResp.rt)
sum(is.na(Exp2_ProlificPilot$preResp.rt))

write.csv(Exp2_ProlificPilot, file = "ProlificPilot.csv")
attach(Exp2_ProlificPilot)



```

Checking the balance in design:

*29 refers to rows containing NA*

```{r design Check}
#blockcount
table(Block)
table(Block,Condition) #29 refers to the NA rows
table(Block,Condition, Validity)

#position
table(PositionD)
table(PositionT)
table(Saliency)

detach(Exp2_ProlificPilot)
```

Cleaning data

```{r cleaning, include=FALSE}

Exp2_ProlificPilot <- Exp2_ProlificPilot %>%
  select(-consentkey.keys,-consentkey.rt,-beginexp.rt,-beginexp.keys,-checkresp.corr,-checkresp.rt,-checkresp.keys,
         -Attention.ran,-Attention.thisIndex,-Attention.thisN,-Attention.thisTrialN,-Attention.thisRepN,
         -Question,-Solution,-gotoPrac.rt,-gotoPrac.keys,-InstRep.ran,-InstRep.thisIndex,-InstRep.thisN,-InstRep.thisTrialN,-InstRep.thisRepN,
         -Prac_start.rt,-Prac_start.keys,-prctrials.ran,-prctrials.thisIndex,-prctrials.thisN,-prctrials.thisRepN,-prctrials.thisRepN,
         -pracend.keys,-pracend.rt,-PracRepeat.ran,-PracRepeat.thisIndex,-PracRepeat.thisN,-PracRepeat.thisRepN,-PracRepeat.thisTrialN)

#removing unwanted columns in the experimental side 
Exp2_ProlificPilot <- Exp2_ProlificPilot %>%
  select(-firstlearntrials.ran,-firstlearntrials.thisIndex,-firstlearntrials.thisRepN,-firstlearntrials.thisTrialN,
         -brkcontinue.keys,-Exptrials.ran,-Exptrials.thisIndex,-Exptrials.thisTrialN, - Exptrials.thisRepN,
         -afterpause.keys,-blocks.ran,-blocks.thisIndex,-blocks.thisTrialN,-blocks.thisRepN,
         -CAproceed.rt,-CAproceed.keys,-ContAwareness.ran,-ContAwareness.thisIndex,-ContAwareness.thisN,-ContAwareness.thisTrialN,-ContAwareness.thisRepN,
         -Questionnaire.ran,-Questionnaire.thisIndex,-Questionnaire.thisTrialN,-Questionnaire.thisRepN,-todebrief.keys,-ExpExit.keys)

#assigning the vaues of blockcount and screenbackground for every row
Exp2_ProlificPilot <- Exp2_ProlificPilot%>%group_by(participant)%>%fill(Screen_bg,.direction = "down")

Exp2_ProlificPilot <- Exp2_ProlificPilot %>% group_by(participant)%>%fill(blocks.thisN,.direction = "up")

Exp2_ProlificPilot <- rename(Exp2_ProlificPilot, c("blocks.thisN" = "BlockCount"))

Exp2_CA <- Exp2_ProlificPilot %>%
  filter(Condition == "ContChkTest" | str_detect(AwareQ, "mit"))

#adjusting RT
Exp2_ProlificPilot <- separate(Exp2_ProlificPilot, col = ResponseKey.rt, into = c("RT_Trials", "RT_secondary"), sep = ',')
Exp2_ProlificPilot$RT_Trials <- Exp2_ProlificPilot$RT_Trials%>%
  str_replace_all("\\[|\\]","")%>%
  as.double(Exp2_ProlificPilot$RT_Trials)
Exp2_ProlificPilot$RT_Trials <- 1000*(Exp2_ProlificPilot$RT_Trials)
Exp2_ProlificPilot$PreTargetDisplayTime <- 1000*(Exp2_ProlificPilot$PreTargetDisplayTime)

Exp2_ProlificPilot <- Exp2_ProlificPilot%>%drop_na(RT_Trials)

Exp2_ProlificPilot$ACC_trials <- Exp2_ProlificPilot$ResponseKey.corr
Exp2_ProlificPilot$ErrorRate <- 1 - Exp2_ProlificPilot$ACC_trials

```

Summary of the overall RT

```{r descriptive, echo=FALSE}
pander(summary(Exp2_ProlificPilot$RT_Trials), style = 'rmarkdown',caption = 'Mean RT')
pander(table(Exp2_ProlificPilot$ACC_trials),style = 'rmarkdown',caption = "Accuracy")

```

Removing outliers

```{r outliersfarouts, echo=FALSE}

Exp2_ProlificPilot$RT_Trials[Exp2_ProlificPilot$ACC_trials==0] <- NA


#creating function to remove the outliers and farouts
computeTukeys <- function(x){
  P25 <- quantile(x$RT_Trials, .25, na.rm = TRUE, type = 6) #type = 6 -> used in SPSS
  P75 <- quantile(x$RT_Trials, .75, na.rm = TRUE, type = 6)
  x$Outlier <- P75 + 1.5*(P75 - P25)
  x$Farouts <- P75 + 3.0*(P75 - P25)
  return(x)
}


#identifying the outliers and farouts at individual level
Exp2_ProlificPilot <- ddply(Exp2_ProlificPilot, .(participant), computeTukeys)

#creating new column with RT trials after removing outliers/farouts
Exp2_ProlificPilot$RT_ifo <- Exp2_ProlificPilot$RT_Trials
Exp2_ProlificPilot$RT_io <- Exp2_ProlificPilot$RT_Trials
Exp2_ProlificPilot$RT_ifo[Exp2_ProlificPilot$RT_ifo > Exp2_ProlificPilot$Farouts|Exp2_ProlificPilot$RT_ifo < 300] <- NA
Exp2_ProlificPilot$RT_io[Exp2_ProlificPilot$RT_io > Exp2_ProlificPilot$Outlier|Exp2_ProlificPilot$RT_io < 300] <- NA

pander(summary(Exp2_ProlificPilot$RT_ifo), style = 'rmarkdown', caption = "Summary of RT after removing Farouts")
pander(summary(Exp2_ProlificPilot$RT_io), style = 'rmarkdown', caption = "Summary of RT after removing Outliers")


```

Split the data as learn and test trials

```{r}
Exp2learn_ProlificPilot <- Exp2_ProlificPilot %>%
  filter(Condition == "learn")

Exp2test_ProlificPilot <- Exp2_ProlificPilot %>%
  filter(Condition == "test")

Exp2learn_ProlificPilot<- Exp2learn_ProlificPilot%>%
  mutate(PositionMatch = ifelse(as.character(PositionD)==as.character(PositionT),"same","different"))

```

## Analysis for learn trials

-   Farouts

```{r learn}
Exp2learn_ProlificPilot <- Exp2learn_ProlificPilot %>%
  select(Validity,Saliency,Condition,PositionD,PositionT,Distractor1,Distractor2,everything())

#aggregate
Exp2agg_l_pp_fo <- aggregate(data = Exp2learn_ProlificPilot,RT_ifo~participant+Validity+Saliency,mean)


t.test(RT_ifo~Validity, data = Exp2agg_l_pp_fo, paired = TRUE)

```

-   Outliers

    ```{r learno}

    #aggregate
    Exp2agg_l_pp_o <- aggregate(data = Exp2learn_ProlificPilot,RT_io~participant+Validity+Saliency,mean)

    t.test(RT_io~Validity, data = Exp2agg_l_pp_o, paired = TRUE)

    ```

-   Error Rate

Why is there a difference of significance in error rate (ANOVA vs t test)
```{r error}

Exp2agg_l_pp_ER <- aggregate(data = Exp2learn_ProlificPilot,ErrorRate~participant+Validity+Saliency,mean)

anova_t_ER <- ezANOVA(data = Exp2agg_l_pp_ER,
        dv = ErrorRate,
        wid = participant,
        within = .(Validity),
        detailed = TRUE)

pander(anova_t_ER, style = "markdown", caption = "ANOVa table for ERROR RATE in learn trials")
t.test(ErrorRate~Validity, data = Exp2agg_l_pp_ER, paired = TRUE)

```

## Analysis of Test Trials

-   Farouts

    There is no main effect of validity: You can see from the summary tables that it is not that different and some are infact slower in the valid condition and faster for invalid. But there seems to be an almost significant interaction between saliency and test trials

```{r test}
Exp2test_ProlificPilot <- Exp2test_ProlificPilot %>%
  select(Validity,Saliency,Condition,PositionD,PositionT,Distractor1,Distractor2,everything())

#aggregate
Exp2agg_t_pp_fo <- aggregate(data = Exp2test_ProlificPilot,RT_ifo~participant+Validity+Saliency,mean)

aggmean_t_fo <- ddply(Exp2agg_t_pp_fo, .(participant,Validity), summarize, RTmean=mean(RT_ifo), SD = sd(RT_ifo))
salmean_t_fo <- ddply(Exp2agg_t_pp_fo, .(participant,Saliency), summarize, RTmean=mean(RT_ifo), SD = sd(RT_ifo))

kable(aggmean_t_fo, format = "html", caption = "Summary means of valid vs invalid across participant")
kable(salmean_t_fo, format = "html", caption = "Summary means of Salient vs non salient across participant")

anova_t_fo <- ezANOVA(data = Exp2agg_t_pp_fo,
        dv = RT_ifo,
        wid = participant,
        within = .(Saliency,Validity),
        detailed = TRUE)

pander(anova_t_fo, style = 'rmarkdown', caption = "ANOVA results: Farouts excluded for test trials")

ezPlot(data = Exp2agg_t_pp_fo,
        dv = RT_ifo,
        wid = participant,
        within = .(Saliency,Validity),
        split = Validity, x=Saliency, do_bars = FALSE)+
        theme_classic()+
        ylim(550,600)+
        ggtitle("Mean RT for valid and invalid trials across saliency")
```

-   Outliers

    This pattern is very different from the pattern from the farouts. There is no interaction anymore and the lines are simply alike, in the opposite direction even. Again we don't see a validity effect nor an interaction effect
   

```{r}
Exp2agg_t_pp_o <- aggregate(data = Exp2test_ProlificPilot,RT_io~participant+Validity+Saliency,mean)
means_t_o <- aggregate(data=Exp2agg_t_pp_o,RT_io~participant+Validity,mean)
salmeans_t_o <- aggregate(data=Exp2agg_t_pp_o,RT_io~participant+Saliency,mean)


kable(means_t_o, format = "html", caption = "Summary means of valid vs invalid across participant")
kable(salmeans_t_o, format = "html", caption = "Summary means of Salient vs non salient across participant")


anova_t_o <- ezANOVA(data = Exp2agg_t_pp_o,
            dv = RT_io,
            wid = participant,
            within = .(Saliency,Validity),
            detailed = TRUE)
pander(anova_t_o, style = 'rmarkdown', caption = "ANOVA results: Outliers excluded for test trials")
 
ezPlot(data = Exp2agg_t_pp_o,
        dv = RT_io,
        wid = participant,
        within = .(Saliency,Validity),
        split = Validity, x=Saliency, do_bars = FALSE)+
  ylim(550,600)+theme_classic()

```

-   Error Rate

    No effects in the error Rates

```{r}

Exp2agg_t_pp_ER <- aggregate(data = Exp2test_ProlificPilot,ErrorRate~participant+Validity+Saliency,mean)

Exp2agg_t_pp_ER$ErrorRate <- 100 * Exp2agg_t_pp_ER$ErrorRate

anova_t_ER <- ezANOVA(data = Exp2agg_t_pp_ER,
        dv = ErrorRate,
        wid = participant,
        within = .(Saliency,Validity),
        detailed = TRUE)

pander(anova_t_ER, style = 'rmarkdown', caption = "ANOVA results: ErrorRates in test trials")


```
### Exploratory: Adding the factor of Position
    
    This analysis discusses the effect of salient word and the position of the number. Position Match variable indicates whether the salient word position and number appeared in the same place or not. Although this may not be of help because we brought the words closer together, I added this factor anyway to see how the data is.

#### Analysis of learn trials - Position
-   Farouts

      NO effect and the RTs seem to be favouring the reversed condition where they are slower when teh position of salient word and number match 

```{r learnp}


#aggregate
Exp2agg_lp_pp_fo <- aggregate(data = Exp2learn_ProlificPilot,RT_ifo~participant+Validity+PositionMatch,mean)


lp_anova <- ezANOVA(data = Exp2agg_lp_pp_fo,
        dv=RT_ifo,
        wid = participant,
        within = .(Validity, PositionMatch),
        detailed = TRUE)
pander(lp_anova,style = 'rmarkdown')

ezPlot(data = Exp2agg_lp_pp_fo,
        dv=RT_ifo,
        wid = participant,
        within = .(Validity, PositionMatch),
       split = Validity, x=PositionMatch, do_bars = FALSE)+
      theme_classic()+
        ylim(550,600)

```

-   Outliers
    
    No Effects

    ```{r learno}
Exp2agg_lp_pp_o <- aggregate(data = Exp2learn_ProlificPilot,RT_io~participant+Validity+PositionMatch,mean)


lp_anova_o <- ezANOVA(data = Exp2agg_lp_pp_o,
        dv=RT_io,
        wid = participant,
        within = .(Validity, PositionMatch),
        detailed = TRUE)
pander(lp_anova_o,style = 'rmarkdown')

ezPlot(data = Exp2agg_lp_pp_o,
        dv=RT_io,
        wid = participant,
        within = .(Validity, PositionMatch),
       split = Validity, x=PositionMatch, do_bars = FALSE)+
        ylim(550,600)+
        theme_classic()

    ```

-   Error Rate

    There is a validity effect.

```{r error}

Exp2agg_lp_pp_ER <- aggregate(data = Exp2learn_ProlificPilot,ErrorRate~participant+Validity+PositionMatch,mean)

lp_anova_er <- ezANOVA(data = Exp2agg_lp_pp_ER,
        dv=ErrorRate,
        wid = participant,
        within = .(Validity, PositionMatch),
        detailed = TRUE)
pander(lp_anova_er,style = 'rmarkdown')
```

#### Analysis of test trials - Position
-   Farouts

    No effect/influence of position of number. 
```{r testp}

#aggregate
Exp2agg_tp_pp_fo <- aggregate(data = Exp2test_ProlificPilot,RT_ifo~participant+Validity+Saliency+PositionT,mean)


tp_anova <- ezANOVA(data = Exp2agg_tp_pp_fo,
        dv=RT_ifo,
        wid = participant,
        within = .(Validity, PositionT),
        detailed = TRUE)
pander(tp_anova,style = 'rmarkdown')

ezPlot(data = Exp2agg_tp_pp_fo,
        dv=RT_ifo,
        wid = participant,
        within = .(Validity, PositionT),
       split = Validity, x=PositionT, do_bars = FALSE)+
  ylim(550,600)+theme_classic()

```

-   Outliers

      Same as farouts, no effects and plot also looks similar (unlike the plot for outliers, without the factor of position that looked different from farouts)

    ```{r testo}
Exp2agg_tp_pp_o <- aggregate(data = Exp2test_ProlificPilot,RT_io~participant+Validity+PositionT,mean)


tp_anova_o <- ezANOVA(data = Exp2agg_tp_pp_o,
        dv=RT_io,
        wid = participant,
        within = .(Validity, PositionT),
        detailed = TRUE)
pander(tp_anova_o,style = 'rmarkdown')

ezPlot(data = Exp2agg_tp_pp_o,
        dv=RT_io,
        wid = participant,
        within = .(Validity, PositionT),
       split = Validity, x=PositionT, do_bars = FALSE)+ylim(550,600)+theme_classic()

    ```

-   Error Rate

    No effect

```{r error}

Exp2agg_tp_pp_ER <- aggregate(data = Exp2test_ProlificPilot,ErrorRate~participant+Validity+PositionT,mean)

tp_anova_er <- ezANOVA(data = Exp2agg_tp_pp_ER,
        dv=ErrorRate,
        wid = participant,
        within = .(Validity, PositionT),
        detailed = TRUE)
pander(tp_anova_er,style = 'rmarkdown')
```
